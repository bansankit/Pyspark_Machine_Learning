{"cells":[{"cell_type":"code","source":["#SparkSession is entrypoint to DataFrames\nfrom pyspark.sql import SparkSession\nmyspark = SparkSession.builder.master(\"local[4]\").appName(\"FirstApp\").config(\"info\",\"great\").getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Create dataframe similar to parallelize\n#Doen't have column information\ndf = spark.createDataFrame([('Awantik',1),('Awi',100)])"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df.collect()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Gets column name from key of dictionary\ndb = [{'name':'Awi','age':50},{'name':'Bwi','ag1':150}]\nspark.createDataFrame(db).collect()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql import Row"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["Person = Row('name','age')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["rdd = sc.parallelize(range(10))\ndf = spark.createDataFrame(rdd)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["l = range(10)\nm = range(20)\n\nrdd = sc.parallelize(zip(l,m))\ndf = spark.createDataFrame(rdd)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["rdd.collect()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df.collect()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Dataframe with only one column\n#Always \nd = [(1,), (2,), (3,)]\nrdd = sc.parallelize(d)\ndf = spark.createDataFrame(rdd)\ndf.collect()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["type((1,))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["db = {'name':'awi','age':50}\ndb.items()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["d = [[1,5], [2,6], [3,8]] \nrdd = sc.parallelize(d)\ndf = spark.createDataFrame(rdd,['age','cost'])\ndf.collect()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["rdd = sc.parallelize([('a',22),('b',44),('c',12)])\nfrom pyspark.sql import Row\nPerson = Row('name','age') #Person is row object\nperson = rdd.map(lambda r: Person(*r))\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df = spark.createDataFrame(person)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def f(*arg):\n  for a in arg:\n    print str(a) + 'hello'\n  \nf(1,2,3)\n\n\nl = (5,6,7)\n\ndef g(x,y,z):\n  print 'x ' + str(x) , 'y ' + str(y), 'z ' + str(z)\n  \ng(*l) # g(5,6,7)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df.toPandas()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#Spark Dataframes are differnt from pandas DataFrame\n#pandas DataFrame is not suited for very large size of data.\n#spark dataframe is solution for large scale data"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#Convert pandas dataframe to spark dataframe\nimport pandas as pd\nspark.createDataFrame(pd.DataFrame([[1,2]]))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["df.collect()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#Create a temp table in current seeion. This table is not available across differnt session\ndf.createOrReplaceTempView(\"tab1\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df2  = spark.sql(\"SELECT name as n, age as a from tab1\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df2.collect()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["spark.sql(\"SELECT name from tab1\").collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["new_df = sqlContext.range(1,20,2)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["new_df.collect()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#Register Function as UDF so that it can be used in SQL statements\nsqlContext.registerFunction(\"myLenFunc\", lambda x:len(x))\nsqlContext.sql(\"SELECT myLenFunc('hello world')\").collect()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["#A PySpark DataType\nfrom pyspark.sql.types import IntegerType\n\nsqlContext.registerFunction(\"myLenFunc\", lambda x:len(x),IntegerType())\nsqlContext.sql(\"SELECT myLenFunc('hello world')\").collect()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["sqlContext.registerDataFrameAsTable(df,\"mytab\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["sqlContext.sql(\"SELECT name from mytab\").collect()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["\"mytab\" in sqlContext.tableNames()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["TrainingData = spark.read.csv('/FileStore/tables/49nfaoyq1489459237361/TrainingDataset.csv',header=True)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["display(TrainingData)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["TrainingData.printSchema()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["startupInfo = spark.read.csv('/FileStore/tables/3mj9uqyq1489459868169/company_master_data_upto_Mar_2015_Karnataka.csv',inferSchema=True,header=True)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["startupInfo.head()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display(startupInfo)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["startupInfo.head(4)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["startupInfo.show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["startupInfo.count()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["startupInfo.columns"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["startupInfo.printSchema()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["#Since dataframe don't have number info, its not showing much here.\n#Otherwise it would have shown - count,mean,stddev,min,max\nstartupInfo.describe()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["startupInfo.select('COMPANY_NAME','DATE_OF_REGISTRATION').show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["type(startupInfo)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["rdd = sc.textFile('/FileStore/tables/3mj9uqyq1489459868169/company_master_data_upto_Mar_2015_Karnataka.csv')"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["rdd.count()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["rdd.first()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["startupInfo.select('COMPANY_CATEGORY').distinct().count()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["startupInfo.select('COMPANY_CATEGORY').distinct().collect()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["startCol = startupInfo['COMPANY_CATEGORY']"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["sales = spark.read.csv('/FileStore/tables/tjx2xm7a1489461154712/Predictor_variables_raw.csv',header=True,inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["sales.printSchema()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["saleCol = sales['sales']"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["saleCol.between(300,500)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["#First sales is your dataframes, second one is column name\nsales.select(sales.sku, sales.date, sales.sales.between(300,500)).show()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["startupInfo.printSchema()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["startupInfo.select( startupInfo.PAIDUP_CAPITAL ).collect()"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["sales_grouped = sales.groupby('sku')"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["sales_grouped.count().collect()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["sales_grouped.max('sales').collect()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["type(spark)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["sales.first()"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["sales.head(4)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["from pyspark.sql import Row"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["l = [('a',1),('b',2),('c',4)]"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["spark.createDataFrame(l).collect()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["r1 = Row(name=\"Awantik\",age=33)\nr2 = Row(name=\"Awantik\",age=35)\ndf = spark.createDataFrame([r1,r2])\ndf.collect()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["df.select('age').map"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["df"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":76}],"metadata":{"name":"PySpark SQL","notebookId":3924684481009833},"nbformat":4,"nbformat_minor":0}
